{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c18d8c6e-cf39-4c5b-8e75-0fbeccce44f2",
   "metadata": {},
   "source": [
    "# Questionnaire Chapter 4: Training a Digit Classifier\n",
    "- [Fastbook Chapter 4 Questionnaire Forum Solutions](https://forums.fast.ai/t/fastbook-chapter-4-questionnaire-solutions-wiki/67253)\n",
    "\n",
    "1. How is a grayscale image represented on a computer? How about a color image?\n",
    "\n",
    "> Images are represented by arrays with pixel values representing the content of the image. For greyscale images, a 2-dimensional array is used with the pixels representing the greyscale values, with a range of 256 integers. A value of 0 would represent white, and a value of 255 represents black, and different shades of greyscale in between. For color images, three color channels (red, green, blue) are typicall used, with a separate 256-range 2D array used for each channel. A pixel value of 0 again represents white, with 255 representing solid red, green, or blue. The three 2-D arrays form a final 3-D array (rank 3 tensor) representing the color image.\n",
    "\n",
    "2. How are the files and folders in the `MNIST_SAMPLE` dataset structured? Why?\n",
    "\n",
    "> The `MNIST` dataset follows a common layout for ML datasets: separate folders for the training set and the validation (and/or test) set. Every practicioner could generate their own train/validation-split of the data. Public datasets are usually pre-split to simplifiy comparing results between implementations/publications.\n",
    ">\n",
    "> fastai `MNIST_SAMPLE` has two subfolders two subsubfolders 3 and 7 which contain the .jpg files for the respective class of images. This is a common way of organizing datasets comprised of pictures. For the full `MNIST` dataset there are 10 subsubfolders, one for the images for each digit.\n",
    "\n",
    "3. Explain how the \"pixel similarity\" approach to classifying digits works.\n",
    "\n",
    "> The main idea is to find the average pixel value for every pixel of the 3s, then do the same for the 7s. This will give two group averages, defining what we might call the \"ideal\" 3 and 7. Then, to classify an image as one digit or the other, we see which of these two ideal digits the image is most similar to.\n",
    "\n",
    "4. What is a list comprehension? Create one now that selects odd numbers from a list and doubles them.\n",
    "\n",
    "> Solution at the end of the notebook.\n",
    "\n",
    "5. What is a \"rank-3 tensor\"?\n",
    "\n",
    "> It's a tensor with 3 dimensions. The rank of a tensor is the number of dimensions it has. In particular, the rank of a tensor is independent of its shape or dimensionality, e.g., a tensor of shape 2x2x2 and a tensor of shape 3x5x7 both have rank 3. Note that the term “rank” has different meanings in the context of tensors and matrices (where it refers to the number of linearly independent column vectors).\n",
    "\n",
    "6. What is the difference between tensor rank and shape? How do you get the rank from the shape?\n",
    "\n",
    "> __*Rank*__ is the number of axes or dimensions in a tensor; __*shape*__ is the size of each axis of a tensor.\n",
    "\n",
    "7. What are RMSE and L1 norm?\n",
    "\n",
    "> Root mean squared error (RMSE), also called the L2 norm, and mean absolute difference (MAE), also called the L1 norm, are two commonly used methods of measuring “distance”. \n",
    ">\n",
    "> In the context of machine learning, the L2 norm penalizes large errors more than small errors, so optimizing with an L2 loss function will reduce the greatest errors, but be insensitive to small errors.\n",
    ">\n",
    "> Similarly, an L2 regularizer will reduce the largest weights. An L1 regularizer is equally incentivized to reduce all weights, so it will drive some to zero while leaving others large. L1 regularization can therefore produce a sparse model.\n",
    "\n",
    "8. How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?\n",
    "\n",
    "> As loops are very slow in Python, it is best to represent the operations as array operations rather than looping through individual elements. If this can be done, then using NumPy or PyTorch will be thousands of times faster, as they use underlying C code which is much faster than pure Python. Even better, PyTorch allows you to run operations on GPU, which will have significant speedup if there are parallel operations that can be done.\n",
    "\n",
    "9. Create a 3×3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers.\n",
    "\n",
    "> Solution at the end of the notebook.\n",
    "\n",
    "10. What is broadcasting?\n",
    "\n",
    "> Scientific/numerical Python packages like NumPy and PyTorch often implement broadcasting which makes code easier to write. In the case of PyTorch, tensors with smaller rank are expanded to have the same size as the larger rank tensor. In this way, operations can be performed between tensors with different rank.\n",
    "\n",
    "11. Are metrics generally calculated using the training set, or the validation set? Why?\n",
    "\n",
    "> We use the __*validation set*__ to calculate metrics because we don't want the model to overfit - that is, train a model to work well only on our training data.\n",
    "\n",
    "12. What is SGD?\n",
    "\n",
    "> Stochastic Gradient Descent (SGD) is an optimization algorithm that updates the parameters of a model to minimize a given loss function that was evaluated on the predictions and target. The main idea behind SGD is that the gradient of the loss function determines the best way to update the parameters to minimize the loss function.\n",
    "\n",
    "13. Why does SGD use mini-batches?\n",
    "\n",
    "> After identifying a loss function to train the network, the next step is to change or update the weights based on the gradients. This is called an optimization step. To take an optimization step, we need to calculate the loss over one or more data items. We can use __*mini-batches*__ to do this.\n",
    "> \n",
    "> __*Mini-batches*__ are a small group of inputs and labels gathered together. It makes the training process quicker and more accurate. Also, we usually use GPUs to train the model. GPUs can perform better if they have lots of computations to do at a time. Thus, it's helpful to give them lots of data items to compute. Using __*mini-batches*__ is one of the best ways to do this.\n",
    ">\n",
    "> However, GPUs can run out of memory if too much work is performed at the same time. Hence, it's a tricky task to choose the right batch size to train a neural network. Thankfully, there are techniques we can use to help us choose the right batch size.\n",
    "\n",
    "14. What are the seven steps in SGD for machine learning?\n",
    "\n",
    "> 1. __*Initialize*__ the weights - random values often work best.\n",
    "> 2. Calculate the __*predictions*__ - this is done on the training set, one mini-batch at a time.\n",
    "> 3. Calculate the __*loss*__ - how good the model is. The average loss over the minibatch is calculated.\n",
    "> 4. Calculate the __*gradient*__ - measures how much changing the weights would optimize the loss function.\n",
    "> 5. __*Step*__ (change) the weights - update the parameters based on the calculated weights.\n",
    "> 6. __*Repeat*__ the process from step 2 until we reach desired output.\n",
    "> 7. __*Stop*__ training - based on time constraints or when the training/validation losses and metrics stop improving.\n",
    "\n",
    "15. How do we initialize the weights in a model?\n",
    "\n",
    "> We initialize the weights to random values.\n",
    "\n",
    "16. What is \"loss\"?\n",
    "\n",
    "> It's a __*function*__ that returns a __*value*__ that represents how well (or badly) the model is doing during training. Lower values correspond to better model predictions.\n",
    "\n",
    "17. Why can't we always use a high learning rate?\n",
    "\n",
    "> The loss may “bounce” around (oscillate) or even diverge, as the optimizer is taking steps that are too large, and updating the parameters faster than it should be.\n",
    "\n",
    "18. What is a \"gradient\"?\n",
    "\n",
    "> The gradient tells the direction in which the weights should be updated to optimize the local minimum of the loss function.\n",
    ">\n",
    "> In other words, the gradients tell us how much we have to change each weight to make our model better. It is essentially a measure of how the loss function changes with changes of the weights of the model (the derivative).\n",
    "\n",
    "19. Do you need to know how to calculate gradients yourself?\n",
    "\n",
    "> Nope, not at all. PyTorch does that for us! This feature is known as automatic differentiation. In PyTorch we can track the gradients using the `requires_grad_()` method.\n",
    "\n",
    "20. Why can't we use accuracy as a loss function?\n",
    "\n",
    "> Because a small change in the weight values is not likely to cause any change in predictions using accuracy. Most of the time the gradients would be 0 if we used accuracy as the loss function.\n",
    ">\n",
    "> In mathematical terms, accuracy is a function that is constant almost everywhere (except at the threshold, 0.5), so its derivative is nil almost everywhere (and infinity at the threshold). This then gives gradients that are 0 or infinite, which are useless for updating the model.\n",
    "\n",
    "21. Draw the sigmoid function. What is special about its shape?\n",
    "\n",
    "> Graph at the end of the notebook.\n",
    ">\n",
    "> The sigmoid function is a smooth curve that takes any input value (positive or negative) and converts into an output value between 0 and 1.\n",
    "\n",
    "22. What is the difference between a loss function and a metric?\n",
    "\n",
    "> The main difference is that metrics drive human understanding and losses drive automated learning. In order for loss to be useful for training, it needs to have a meaningful derivative. Many metrics, like accuracy, are not like that. Metrics instead are the numbers that humans care about, that reflect the performance of the model.\n",
    "\n",
    "23. What is the function to calculate new weights using a learning rate?\n",
    "\n",
    "> The optimizer step function.\n",
    "\n",
    "24. What does the `DataLoader` class do?\n",
    "\n",
    "> The DataLoader class can take any Python collection and turn it into an iterator over many batches.\n",
    "\n",
    "25. Write pseudocode showing the basic steps taken in each epoch for SGD.\n",
    "\n",
    "        for x,y in dl:\n",
    "        pred = model(x)                     # make predictions\n",
    "        loss = loss_func(pred, y)           # calculate loss\n",
    "        loss.backward()                     # calculate gradients\n",
    "        parameters -= parameters.grad * lr  # update parameters\n",
    "\n",
    "\n",
    "26. Create a function that, if passed two arguments `[1,2,3,4]` and `'abcd'`, returns `[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]`. What is special about that output data structure?\n",
    "\n",
    "> Solution at the end of the notebook.\n",
    "\n",
    "27. What does `view` do in PyTorch?\n",
    "\n",
    "> Returns a new tensor with the same data as the `self` tensor but of a different `shape`.\n",
    "\n",
    "28. What are the \"bias\" parameters in a neural network? Why do we need them?\n",
    "\n",
    "> Without the bias parameters, if the input is zero, the output will always be zero. Therefore, using bias parameters adds additional flexibility to the model.\n",
    "\n",
    "29. What does the `@` operator do in Python?\n",
    "\n",
    "> Performs matrix multiplication.\n",
    "\n",
    "30. What does the `backward` method do?\n",
    "\n",
    "> Computes the gradient of the loss with respect to all model parameters.\n",
    "\n",
    "31. Why do we have to zero the gradients?\n",
    "\n",
    "> PyTorch will add the gradients of a variable to any previously stored gradients. If the training loop function is called multiple times, without zeroing the gradients, the gradient of current `loss` would be added to the previously stored gradient value.\n",
    "\n",
    "32. What information do we have to pass to `Learner`?\n",
    "\n",
    "> We need to pass in the DataLoaders, the model, the optimization function, the loss function, and any metrics to print.\n",
    "\n",
    "\n",
    "        learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,\n",
    "                        loss_func=mnist_loss, metrics=batch_accuracy)\n",
    "\n",
    "\n",
    "33. Show Python or pseudocode for the basic steps of a training loop.\n",
    "\n",
    "        def train_epoch(model, lr, params):\n",
    "            for xb,yb in dl:\n",
    "                calc_grad(xb, yb, model)\n",
    "                for p in params:\n",
    "                    p.data -= p.grad*lr\n",
    "                    p.grad.zero_()\n",
    "        for i in range(20):\n",
    "            train_epoch(model, lr, params)\n",
    "                \n",
    "\n",
    "34. What is \"ReLU\"? Draw a plot of it for values from `-2` to `+2`.\n",
    "\n",
    "> Rectified Linear Unit (ReLu) is an activation function that returns 0 for negative values and doesn't change positive values.\n",
    "\n",
    "35. What is an \"activation function\"?\n",
    "\n",
    "> An activation function is a nonlinear function used to train a neural network by defining the output of a neuron given a\n",
    "set of inputs. This function is used in every neuron of the network to help the network learn the complex patterns in\n",
    "the data, allowing it to make predictions. Moreover, the purpose of activation functions is to introduce nonlinearities\n",
    "into the neural network.\n",
    ">\n",
    "> An artificial neural network is composed of a large number of connected nodes known as perceptrons or neurons. A perceptron (figure below) receives real-valued numerical inputs of a feature (x) and multiplies each input with its associated weights (w) and bias (b) to compute the net input, which is the sum of all connected neurons. The result of net input is passed on to the activation function that mathematically transforms the output into a range of values (e.g., the sigmoid function transforms the inputs into a range between 0 and 1).\n",
    ">\n",
    "> <img src=\"https://i.stack.imgur.com/Waz75.png\" width=700 height=700 />\n",
    ">\n",
    "> For more information, check it out the [survey paper](https://arxiv.org/abs/2204.02921) I wrote about activation functions.\n",
    "\n",
    "36. What's the difference between `F.relu` and `nn.ReLU`?\n",
    "\n",
    "> `F.relu` is a Python function for the relu activation function. On the other hand, `nn.ReLU` is a PyTorch module. This means that it is a Python class that can be called as a function in the same way as `F.relu`.\n",
    "\n",
    "37. The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more?\n",
    "\n",
    "> There are practical performance benefits to using more than one nonlinearity. We can use a deeper model with less number of parameters, better performance, faster training, and less compute/memory requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c2efa10-b5ca-4488-921c-4a8ef9b2a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastbook import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60883344-48d1-4c2b-8007-fc271325938f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 6, 10, 14, 18]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Create a list comprehension that selects odd numbers from a list and doubles them.\n",
    "# List of numbers\n",
    "nums = range(10)\n",
    "\n",
    "# Double odd numbers\n",
    "double_odd = [2*n for n in nums if n % 2 != 0]\n",
    "double_odd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "411103fb-1f8f-42fe-94a2-65cbe905e117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]] \n",
      "\n",
      "[[ 2  4  6]\n",
      " [ 8 10 12]\n",
      " [14 16 18]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[10, 12],\n",
       "       [16, 18]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9. Create a 3×3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers.\n",
    "import numpy as np\n",
    "\n",
    "array = np.array(range(1, 10)).reshape(3, 3)\n",
    "print(array, '\\n')\n",
    "\n",
    "array = array*2\n",
    "print(array, '\\n')\n",
    "\n",
    "array[1:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3e39219-c95e-4c80-be91-80bb47a7b37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]]) \n",
      "\n",
      "tensor([[ 2.,  4.,  6.],\n",
      "        [ 8., 10., 12.],\n",
      "        [14., 16., 18.]]) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[10., 12.],\n",
       "        [16., 18.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.Tensor(range(1, 10)).view(3,3)\n",
    "print(tensor, '\\n')\n",
    "\n",
    "tensor = 2*tensor\n",
    "print(tensor, '\\n')\n",
    "\n",
    "tensor[1:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7226b72-9609-4413-a66f-518f04cc8617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 21. Draw the sigmoid function.\n",
    "def sigmoid(x): return 1/(1+torch.exp(-x))\n",
    "\n",
    "plot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37834f1f-1c97-46f9-96dd-410d382437dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 26. Create a function...\n",
    "def func(a,b): return list(zip(a,b))\n",
    "\n",
    "nums = range(1, 5)\n",
    "letters = 'abcd'\n",
    "func(nums, letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a31763-5d04-46c0-923d-2e385b28703d",
   "metadata": {},
   "source": [
    "### Deep learning terminology "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1e48be-8a5e-42ec-854d-74010aec4fae",
   "metadata": {},
   "source": [
    "| Term | Meaning |\n",
    "| ---- | ------- |\n",
    "|ReLU | Function that returns 0 for negative numbers and doesn't change positive numbers. |\n",
    "|Mini-batch | A small group of inputs and labels gathered together in two arrays. A gradient descent step is updated on this batch (rather than a whole epoch). |\n",
    "|Forward pass | Applying the model to some input and computing the predictions. |\n",
    "|Loss | A value that represents how well (or badly) our model is doing. |\n",
    "|Gradient | The derivative of the loss with respect to some parameter of the model. |\n",
    "|Backward pass | Computing the gradients of the loss with respect to all model parameters. |\n",
    "|Gradient descent | Taking a step in the directions opposite to the gradients to make the model parameters a little bit better. |\n",
    "|Learning rate | The size of the step we take when applying SGD to update the parameters of the model. |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
